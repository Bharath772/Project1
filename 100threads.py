# -*- coding: utf-8 -*-
"""100threads.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gZgxal8cXguTrjDFCo-naiq5my4Uj1P2
"""

from google.colab import drive
drive.mount('/content/drive')

"""**Understanding the Data**


The data set consists of 2 files:

train.csv: This file will be used to build the model
test.csv: This file will be used to predict the purchase
The data set consists of following Columns:

• User_ID : User id of the customer

• Product_ID: Product id of the product

• Gender: male or female

• Age: Age in bins i.e 0-17, 18-25, 26-35, 36-45, 46-50, 51-55, 55+

• Occupation: Occupation (Masked)

• City_Category: Category of the City (A,B,C)

• Stay_In_Current_City_Years: Number of years stay in current city

• Marital_Status: 0-Unmarried, 1-Married

• Product_Category_1: Product Category (Masked)

• Product_Category_2: Product may belongs to other category also (Masked)

• Product_Category_3: Product may belongs to other category also (Masked)

Analysis step
Trying to identify the most important variables and defining the best regression model for predicting target variable. Hence, this analysis will be divided into five stages:

**1.Exploratory data analysis (EDA)**

**2.Data Pre-processing**

**3.Feature engineering**

**4.Modeling**

**5.Prediction and Metrics**

**6.Improving the Model (Hyperparameter tuning)**
"""

import numpy as np
import pandas as pd

train=pd.read_csv('/content/drive/MyDrive/Sales Data/train.csv')
test=pd.read_csv('/content/drive/MyDrive/Sales Data/test.csv')
train.head()

train.describe()

train.isna().sum()

train.isna().mean()*100

# droping Product_Category_3 column
train.drop(["Product_Category_3"],  axis=1, inplace=True)

train.head()

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

plt.style.use('fivethirtyeight')
plt.figure(figsize=(15,7))
sns.histplot(train.Purchase, bins = 25)
plt.xlabel("Amount spent in Purchase")
plt.ylabel("Number of Buyers")
plt.title("Purchase amount Distribution")

numeric_features = train.select_dtypes(include=[np.number])
numeric_features.dtypes

sns.countplot(x="Marital_Status",data=train)

sns.countplot(x="Product_Category_1",data=train)

sns.countplot(x="Product_Category_2",data=train)
plt.xticks(rotation=90)

corr = numeric_features.corr()

#correlation matrix
f, ax = plt.subplots(figsize=(20, 9))
sns.heatmap(corr,  annot=True,annot_kws={'size': 15})

sns.countplot(x="Gender",data=train)

sns.countplot(x="Age",data=train)

sns.countplot(x="City_Category",data=train)

marital_status_pivot= train.pivot_table(index='Marital_Status',values='Purchase', aggfunc=np.mean)
marital_status_pivot

marital_status_pivot.plot(kind='bar', color='lightgreen',figsize=(12,7))
plt.xlabel("Marital_Status")
plt.ylabel("Purchase")
plt.title("Marital_Status and Purchase Analysis")
plt.xticks(rotation=0)
plt.show()

Product_category_1_pivot = train.pivot_table(index='Product_Category_1', values="Purchase", aggfunc=np.mean)
Product_category_1_pivot

Product_category_1_pivot.plot(kind='bar', color='green',figsize=(12,7))
plt.xlabel("Product_Category_1")
plt.ylabel("Purchase")
plt.title("Product_Category_1 and Purchase Analysis")
plt.xticks(rotation=0)
plt.show()

Product_category_2_pivot = train.pivot_table(index='Product_Category_2', values="Purchase", aggfunc=np.mean)
Product_category_2_pivot

Product_category_2_pivot.plot(kind='bar', color='brown',figsize=(12,7))
plt.xlabel("Product_Category_2")
plt.ylabel("Purchase")
plt.title("Product_Category_2 and Purchase Analysis")
plt.xticks(rotation=0)
plt.show()

gender_pivot = train.pivot_table(index='Gender', values="Purchase", aggfunc=np.mean)
gender_pivot

gender_pivot.plot(kind='bar', color='pink',figsize=(12,7))
plt.xlabel("Gender")
plt.ylabel("Purchase")
plt.title("Gender and Purchase Analysis " "AVERAGE")
plt.xticks(rotation=0)
plt.show()

age_pivot = train.pivot_table(index='Age', values="Purchase", aggfunc=np.sum)
age_pivot

age_pivot.plot(kind='bar', color='pink',figsize=(12,7))
plt.xlabel("Age")
plt.ylabel("Purchase")
plt.title("Age and Purchase Analysis " "AVERAGE")
plt.xticks(rotation=0)
plt.show()

Stay_In_Current_City_Years_pivot = train.pivot_table(index='Stay_In_Current_City_Years', values="Purchase", aggfunc=np.mean)
Stay_In_Current_City_Years_pivot

Stay_In_Current_City_Years_pivot.plot(kind='bar', color='red',figsize=(12,7))
plt.xlabel("Stay_in_Current_City_Years")
plt.ylabel("Purchase")
plt.title("Stay_in_Current_City_Years and Purchase Analysis")
plt.xticks(rotation=0)
plt.show()

"""Again, we see the same pattern seen before which show that on average people tend to spend the same amount on purchases regardeless of their group. People who are new in city are responsible for the higher number of purchase, however looking at it individually they tend to spend the same amount independently of how many years the have lived in their current city.

**Data Pre-Processing**
"""

test.head()

"""It is generally a good idea to combine both test and train sets into one, in order to perform data cleaning and feature engineering and later divide them again. With this step we do not have to go through the trouble of repeating twice the same code, for both datasets. Let’s combine them into a dataframe datawith a source column specifying where each observation belongs."""

# Join Train and Test Dataset
train['source']='train'
test['source']='test'

df = pd.concat([train,test], ignore_index = True, sort = False)

print(train.shape, test.shape, df.shape)

"""Since train set do not contain column product_category_3 , it has to be deleted from test as well as combined data frame"""

test.drop(["Product_Category_3"],  axis=1, inplace=True)
df.drop(["Product_Category_3"],  axis=1, inplace=True)

print(train.shape, test.shape, df.shape)

"""## **Dealing with Null-Values**"""

#Check the percentage of null values per variable
df.isnull().mean()*100

# Replacing Null Values in Product_Category_2 with the median of the column
df["Product_Category_2"].fillna(train["Product_Category_2"].median(), inplace = True)

"""Removing Product_Category_1 group 19 and 20 from Train as this is not in Product_Category_2"""

#Get index of all columns with product_category_1 equal 19 or 20 from train

ind = df.index[(df.Product_Category_1.isin([19,20])) & (df.source == "train")]
df = df.drop(ind)

df.shape

"""**Dealing with Categorical Values**"""

df.dtypes

"""## The categorical columns are Product_ID, Gender, Age, City_Category, Stay_In_Current_City_Years and Source"""

#Filter categorical variables and get dataframe will all strings columns names except Item_identfier and outlet_identifier
category_cols = df.select_dtypes(include=['object']).columns.drop(["source"])
#Print frequency of categories
for col in category_cols:
    #Number of times each value appears in the column
    frequency = df[col].value_counts()
    print("\nThis is the frequency distribution for " + col + ":")
    print(frequency)

"""# Feature Engineering

# Converting gender to binary
"""

gender_dict = {'F':0, 'M':1}
df["Gender"] = df["Gender"].apply(lambda x: gender_dict[x])

df["Gender"].value_counts()

"""# Converting Age to numeric values"""

age_dict={'0-17':0, '18-25':1, '26-35':2, '36-45':3, '46-50':4, '51-55':5, '55+':6}
df['Age']=df['Age'].apply(lambda x:age_dict[x])
df['Age'].value_counts()

"""# Converting city_category to Numeric"""

city={'A':0,'B':1,'C':2}
df['City_Category']=df['City_Category'].apply(lambda x: city[x])
df['City_Category'].value_counts()

"""# Converting Stay_In_Current_City_Year to numeric"""

def stay(Stay_In_Current_City_Years):
        if Stay_In_Current_City_Years == '4+':
            return 4
        else:
            return Stay_In_Current_City_Years
df['Stay_In_Current_City_Years'] = df['Stay_In_Current_City_Years'].apply(stay).astype(int)

"""# Exporting Data"""

#Divide into test and train:
train = df.loc[df['source']=="train"]
test = df.loc[df['source']=="test"]

#Drop unnecessary columns:
test.drop(['source'],axis=1,inplace=True)
train.drop(['source'],axis=1,inplace=True)

#Export files as modified versions:
train.to_csv("train_clean.csv",index=False)
test.to_csv("test_clean.csv",index=False)

train= pd.read_csv('/content/drive/MyDrive/Sales Data/train_clean.csv')
train.head()

test= pd.read_csv('/content/drive/MyDrive/Sales Data/test_clean.csv')
test.head()

"""# Modelling"""

X = train.drop(['Product_ID','User_ID','Purchase'], axis=1)
y = train["Purchase"]

# splitting train and test set
X_train, X_test, y_train, y_test= train_test_split(X,y,test_size=0.2, random_state=42)

"""In order to identify the best model,we will compare the following ml algorithms:

Random Forest Regressor

Linear Regression

Decision Tree Regressor

# Random Forest Regressor Model
"""

# Commented out IPython magic to ensure Python compatibility.
#model
# %time
rf_regressor = RandomForestRegressor(n_jobs=-1,
                              random_state=42)

rf_regressor.fit(X_train, y_train)

# Commented out IPython magic to ensure Python compatibility.
#model
# %time
lr_regressor=LinearRegression(normalize=True, n_jobs=-1)

lr_regressor.fit(X_train, y_train)

"""# Decision Tree Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# model
# %time
DT_regressor = DecisionTreeRegressor()
DT_regressor.fit(X_train, y_train)

"""# Prediction and Metrics"""

def pred(mod,X_test,y_test):
  y_pred=mod.predict(X_test)
  R2 = r2_score(y_test, y_pred)
  RMSE = np.sqrt(mean_squared_error(y_test, y_pred))
  return R2, RMSE

"""# Random Forest Regressor"""

r2_score_rf, rmse_rf= pred(rf_regressor,X_test,y_test)

"""# Linear Regression"""

r2_score_lr, rmse_lr= pred(lr_regressor,X_test,y_test)

"""# Decision Tree Regressor"""

r2_score_dt, rmse_dt= pred(DT_regressor,X_test,y_test)

compare = {"R^2_score":[r2_score_rf,r2_score_dt],
            " Root Mean Squared Error": [rmse_rf, rmse_dt]}


Compare = pd.DataFrame(compare, index=[["Random Forest Regressor","Decision Tree Regressor"]])
Compare

"""It seems that among all the two models, Random Forest Regressor has highest R^2_score and lowest Root Mean Squared Error.

Thus Random Forest Regressor is performs best.

**Improving Model**
"""

grid =  {"n_estimators": [10,50,100],
       "max_depth": [None,10,20,30,40,50,],
       "max_features": ["auto", "sqrt"],
       "min_samples_leaf": [2,10,15],
       "min_samples_split": [2,5,20]}

randomsearchCV = RandomizedSearchCV(rf_regressor, param_distributions = grid, n_iter = 5, cv=5,  verbose = True, n_jobs=-1)

randomsearchCV.best_params_

rf_regressor_tune = RandomForestRegressor(n_estimators=100, max_depth = 50, max_features = 'auto', min_samples_leaf =10,
                                     min_samples_split=2)

rf_regressor_tune.fit(X_train, y_train)

r2_rf_tune, rmse_rf_tune=pred(rf_regressor_tune,X_test,y_test)

compare1 = {"R^2_score":[r2_score_rf, r2_rf_tune],
            " Root Mean Squared Error": [rmse_rf,rmse_rf_tune]}


Compare1 = pd.DataFrame(compare1, index=[["Before_tune", "After_tune",]])
Compare1

"""**Make predictions on test data with the model whose hyperparameters are tuned**"""

test.head()

predicted= test[['User_ID','Product_ID']]
test =test.drop(['User_ID','Product_ID','Purchase'],axis=1)

test.head()

test_pred = rf_regressor_tune.predict(test)
test_pred

print(rf_regressor_tune.feature_importances_)

columns = pd.DataFrame({"Features": test.columns,
                        "Feature Importance" :rf_regressor_tune.feature_importances_})

columns.sort_values("Feature Importance", ascending = False).reset_index(drop=True)

sns.barplot(y="Features", x = "Feature Importance", data = columns)

predicted['Predicted_Purchase']=test_pred

predicted.head()

predicted.to_csv("predict.csv",index=False)

import pickle
pickle_file = open('model.pkl', 'ab')
pickle.dump(rf_regressor_tune, pickle_file)
pickle_file.close()

!pip install gradio==3.43.1

import gradio as gr
import pandas as pd
import pickle
from datetime import date,timedelta,datetime

import gradio as gr
import pandas as pd
from datetime import datetime
import pickle

def stay(Stay_In_Current_City_Years):
    if Stay_In_Current_City_Years == '4+':
        return 4
    else:
        return Stay_In_Current_City_Years

def fn(inputs):
    df=pd.read_csv(inputs.name)
    df=df.iloc[:10,:]
    df.drop(["Product_Category_3"],  axis=1, inplace=True)
    df["Product_Category_2"].fillna(df["Product_Category_2"].median(), inplace = True)
    gender_dict = {'F':0, 'M':1}
    df["Gender"] = df["Gender"].apply(lambda x: gender_dict[x])
    age_dict={'0-17':0, '18-25':1, '26-35':2, '36-45':3, '46-50':4, '51-55':5, '55+':6}
    df['Age']=df['Age'].apply(lambda x:age_dict[x])
    city={'A':0,'B':1,'C':2}
    df['City_Category']=df['City_Category'].apply(lambda x: city[x])
    df['Stay_In_Current_City_Years'] = df['Stay_In_Current_City_Years'].apply(stay).astype(int)
    predicted= df[['User_ID','Product_ID']]
    df=df.drop(['User_ID','Product_ID'],axis=1)
    with open('model.pkl', 'rb') as pickle_file:
        model = pickle.load(pickle_file)
    test_pred = model.predict(df)
    predicted['Predicted_Purchase']=test_pred
    filename1 = datetime.now().strftime("%Y_%m_%d")
    predicted.to_csv(filename1+"_predict.csv",index=False)
    return predicted

file = gr.inputs.File(label=None)
output = gr.outputs.Dataframe(type="pandas")
gui = gr.Interface(fn=fn,
                   inputs=file,
                   outputs=output,
                   title="Future Sales Prediction Using \n Machine Learning",
                   article="")
gui.launch(share=True)